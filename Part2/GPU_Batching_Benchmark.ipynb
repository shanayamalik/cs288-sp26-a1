{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02b6b27",
   "metadata": {},
   "source": [
    "# MLP Batching Benchmark (GPU Required)\n",
    "\n",
    "**CS288 Assignment 1 - Part 2**\n",
    "\n",
    "This notebook runs batching experiments to measure GPU speedup with different batch sizes.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ IMPORTANT: Enable GPU First!\n",
    "\n",
    "**Before running any cells:**\n",
    "1. Click **Runtime** â†’ **Change runtime type**\n",
    "2. Set **Hardware accelerator** to **GPU (T4)**\n",
    "3. Click **Save**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd1c69",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50988517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/shanayamalik/cs288-sp26-a1.git\n",
    "%cd cs288-sp26-a1/Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8b8fa",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbae071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414ee24",
   "metadata": {},
   "source": [
    "## Step 3: Verify GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2584180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âŒ GPU NOT AVAILABLE - Go to Runtime -> Change runtime type -> GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70d3dc",
   "metadata": {},
   "source": [
    "## Step 4: Run Batching Benchmark\n",
    "\n",
    "This will test batch sizes: 1, 4, 8, 16, 32, 64, 128\n",
    "\n",
    "Each batch size is tested 5 times to get mean and standard deviation.\n",
    "\n",
    "**This will take ~5-10 minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb185031",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark_batching.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1484f9e",
   "metadata": {},
   "source": [
    "## ðŸ“Š Interpreting Results\n",
    "\n",
    "The output shows:\n",
    "- **Batch Size**: Number of examples processed together\n",
    "- **Mean Time (s)**: Average seconds to process 1,000 examples\n",
    "- **Std Dev (s)**: Standard deviation across 5 runs\n",
    "- **Speedup**: How much faster compared to batch_size=1\n",
    "\n",
    "**Expected Observations:**\n",
    "1. Batch size 1 (no batching) is slowest\n",
    "2. Larger batches show significant speedup (3-10x)\n",
    "3. Speedup plateaus at some batch size due to GPU memory/computation limits\n",
    "\n",
    "**Copy the results table for your writeup!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbff39b",
   "metadata": {},
   "source": [
    "## Optional: Run Additional Experiments\n",
    "\n",
    "You can modify the benchmark script to test different configurations:\n",
    "- Different datasets (newsgroups instead of sst2)\n",
    "- More batch sizes\n",
    "- Different model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: View the benchmark script\n",
    "!head -50 benchmark_batching.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
