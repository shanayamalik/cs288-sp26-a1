{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862b062c",
   "metadata": {},
   "source": [
    "# MLP Activation Function Comparison\n",
    "\n",
    "**CS288 Assignment 1 - Part 2**\n",
    "\n",
    "This notebook compares ReLU, Sigmoid, and Tanh activation functions to find the best one.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Optional: Enable GPU for Faster Training\n",
    "\n",
    "**GPU is recommended but not required:**\n",
    "- With GPU: ~5-10 minutes total\n",
    "- Without GPU: ~30-45 minutes total\n",
    "\n",
    "To enable GPU:\n",
    "1. Click **Runtime** â†’ **Change runtime type**\n",
    "2. Set **Hardware accelerator** to **GPU (T4)**\n",
    "3. Click **Save**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b63761",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/shanayamalik/cs288-sp26-a1.git\n",
    "%cd cs288-sp26-a1/Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c1fa6",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804cf70",
   "metadata": {},
   "source": [
    "## Step 3: Check Device (GPU or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Training will be fast (~5-10 minutes)\")\n",
    "else:\n",
    "    print(\"âš ï¸  Running on CPU - training will be slower (~30-45 minutes)\")\n",
    "    print(\"   Consider enabling GPU in Runtime -> Change runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0f2a4",
   "metadata": {},
   "source": [
    "## Step 4: Run Activation Function Comparison\n",
    "\n",
    "This will train 3 models (one for each activation function) and compare their performance.\n",
    "\n",
    "Each model trains for 3 epochs on SST-2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1abd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python compare_activations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d4987",
   "metadata": {},
   "source": [
    "## ðŸ“Š Interpreting Results\n",
    "\n",
    "The output shows:\n",
    "- **Train Acc**: Accuracy on training set (should be high, ~85%+)\n",
    "- **Val Acc**: Accuracy on validation set during training\n",
    "- **Dev Acc**: Accuracy on development set (the key metric)\n",
    "\n",
    "**Expected Patterns:**\n",
    "1. **ReLU** typically performs best (fastest convergence, no vanishing gradient)\n",
    "2. **Sigmoid** may have lower accuracy (saturates at extremes)\n",
    "3. **Tanh** usually between ReLU and Sigmoid\n",
    "\n",
    "**Use the activation function with the highest Dev Acc for final predictions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9d1d5",
   "metadata": {},
   "source": [
    "## Step 5: Generate Final Predictions with Best Activation\n",
    "\n",
    "Based on the results above, run this cell with the best activation function.\n",
    "\n",
    "**Replace `BEST_ACTIVATION` below with the winner (relu, sigmoid, or tanh):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'relu' with the best activation from above results\n",
    "BEST_ACTIVATION = 'relu'  # Change this to 'sigmoid' or 'tanh' if needed\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Generating predictions with {BEST_ACTIVATION.upper()} activation\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Train on SST-2 with best activation\n",
    "!python multilayer_perceptron.py -d sst2 -a {BEST_ACTIVATION} -e 3\n",
    "\n",
    "print(f\"\\nâœ… Generated: results/mlp_sst2_test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dd1fe",
   "metadata": {},
   "source": [
    "## Step 6: Download Results\n",
    "\n",
    "Download the predictions CSV file to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the predictions file\n",
    "files.download('results/mlp_sst2_test_predictions.csv')\n",
    "\n",
    "print(\"\\nâœ… Download complete! Check your browser's downloads folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f012d",
   "metadata": {},
   "source": [
    "## ðŸ“ For Your Writeup\n",
    "\n",
    "Copy the comparison table from Step 4 and include:\n",
    "\n",
    "> \"Experimented with three activation functions (ReLU, Sigmoid, Tanh) as specified in the assignment. ReLU achieved the best development accuracy of X.XX%, outperforming Sigmoid (X.XX%) and Tanh (X.XX%). This aligns with ReLU's advantages in deep learning: faster convergence and reduced vanishing gradient problems. The final MLP model uses ReLU activation.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Optional: Test on Newsgroups Dataset\n",
    "\n",
    "If you want to compare activations on newsgroups too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b95071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test activations on newsgroups\n",
    "# !python multilayer_perceptron.py -d newsgroups -a relu -e 3\n",
    "# !python multilayer_perceptron.py -d newsgroups -a sigmoid -e 3\n",
    "# !python multilayer_perceptron.py -d newsgroups -a tanh -e 3"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
