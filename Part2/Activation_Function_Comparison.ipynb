{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862b062c",
   "metadata": {},
   "source": [
    "# MLP Activation Function Comparison\n",
    "\n",
    "**CS288 Assignment 1 - Part 2**\n",
    "\n",
    "This notebook compares ReLU, Sigmoid, and Tanh activation functions with **10 epochs** to find the best one.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Optional: Enable GPU for Faster Training\n",
    "\n",
    "**GPU is recommended but not required:**\n",
    "- With GPU: ~15-20 minutes total\n",
    "- Without GPU: ~1-2 hours total\n",
    "\n",
    "To enable GPU:\n",
    "1. Click **Runtime** â†’ **Change runtime type**\n",
    "2. Set **Hardware accelerator** to **GPU (T4)**\n",
    "3. Click **Save**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b63761",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/shanayamalik/cs288-sp26-a1.git\n",
    "%cd cs288-sp26-a1/Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c1fa6",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804cf70",
   "metadata": {},
   "source": [
    "## Step 3: Check Device (GPU or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Training will be fast (~5-10 minutes)\")\n",
    "else:\n",
    "    print(\"âš ï¸  Running on CPU - training will be slower (~30-45 minutes)\")\n",
    "    print(\"   Consider enabling GPU in Runtime -> Change runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0f2a4",
   "metadata": {},
   "source": [
    "## Step 4: Train with ReLU Activation (10 Epochs)\n",
    "\n",
    "Training first model with ReLU activation function for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1abd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Training with ReLU activation (10 epochs)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python multilayer_perceptron.py -d sst2 -a relu -e 10\n",
    "\n",
    "print(\"\\nâœ… ReLU training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaaff03",
   "metadata": {},
   "source": [
    "## Step 5: Train with Sigmoid Activation (10 Epochs)\n",
    "\n",
    "Training second model with Sigmoid activation function for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414933e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Training with Sigmoid activation (10 epochs)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python multilayer_perceptron.py -d sst2 -a sigmoid -e 10\n",
    "\n",
    "print(\"\\nâœ… Sigmoid training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c498a",
   "metadata": {},
   "source": [
    "## Step 6: Train with Tanh Activation (10 Epochs)\n",
    "\n",
    "Training third model with Tanh activation function for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee12432",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Training with Tanh activation (10 epochs)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python multilayer_perceptron.py -d sst2 -a tanh -e 10\n",
    "\n",
    "print(\"\\nâœ… Tanh training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee104a",
   "metadata": {},
   "source": [
    "## Step 7: Compare All Results\n",
    "\n",
    "Now manually compare the **Development accuracy** from all three runs above:\n",
    "\n",
    "| Activation | Dev Accuracy |\n",
    "|------------|-------------|\n",
    "| ReLU       | ___.__%     |\n",
    "| Sigmoid    | ___.__%     |\n",
    "| Tanh       | ___.__%     |\n",
    "\n",
    "**Fill in the table with the Dev accuracy values from each training run above.**\n",
    "\n",
    "The activation function with the **highest Dev accuracy** should be used for final predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d4987",
   "metadata": {},
   "source": [
    "## ðŸ“Š How to Interpret Results\n",
    "\n",
    "For each training run above, look at the **final line** showing:\n",
    "- **Training accuracy**: How well the model fits the training data\n",
    "- **Validation accuracy**: Performance during training\n",
    "- **Development accuracy**: **THE KEY METRIC** - this determines which activation wins!\n",
    "\n",
    "**What to watch for:**\n",
    "1. **Overfitting**: Large gap between Training and Dev accuracy (e.g., 96% train, 74% dev = 22% gap)\n",
    "2. **Better generalization**: Smaller gaps indicate the model generalizes better\n",
    "3. **Final Dev accuracy**: The activation with highest Dev accuracy is the winner!\n",
    "\n",
    "**Expected patterns with 10 epochs:**\n",
    "- Models should converge (validation accuracy plateaus)\n",
    "- May see which activation handles longer training better\n",
    "- ReLU might overfit more, Sigmoid/Tanh might generalize better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9d1d5",
   "metadata": {},
   "source": [
    "## Step 8: Generate Final Predictions with Best Activation\n",
    "\n",
    "Based on the comparison in Step 7, use the activation with the **highest Dev accuracy**.\n",
    "\n",
    "**Replace `BEST_ACTIVATION` below with the winner:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the best activation from Step 7\n",
    "BEST_ACTIVATION = 'sigmoid'  # Change this based on your results (relu, sigmoid, or tanh)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Generating predictions with {BEST_ACTIVATION.upper()} activation (10 epochs)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Train on SST-2 with best activation for 10 epochs\n",
    "!python multilayer_perceptron.py -d sst2 -a {BEST_ACTIVATION} -e 10\n",
    "\n",
    "print(f\"\\nâœ… Generated: results/mlp_sst2_test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dd1fe",
   "metadata": {},
   "source": [
    "## Step 9: Download Results\n",
    "\n",
    "Download the predictions CSV file to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the predictions file\n",
    "files.download('results/mlp_sst2_test_predictions.csv')\n",
    "\n",
    "print(\"\\nâœ… Download complete! Check your browser's downloads folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f012d",
   "metadata": {},
   "source": [
    "## ðŸ“ For Your Writeup (Step 10)\n",
    "\n",
    "Use the comparison table from Step 7 and include in your writeup:\n",
    "\n",
    "> \"Experimented with three activation functions (ReLU, Sigmoid, Tanh) as specified in the assignment, training each for 10 epochs. Results showed:\n",
    "> - ReLU: X.XX% dev accuracy\n",
    "> - Sigmoid: X.XX% dev accuracy  \n",
    "> - Tanh: X.XX% dev accuracy\n",
    ">\n",
    "> [Winning activation] achieved the best development accuracy. [Describe any patterns you observed, such as: 'ReLU showed signs of overfitting with high training accuracy but lower dev accuracy, while Sigmoid generalized better with a smaller train-dev gap.']\n",
    ">\n",
    "> The final MLP model uses [winning activation] activation for test predictions.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Optional: Test on Newsgroups Dataset\n",
    "\n",
    "If you want to compare activations on newsgroups too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b95071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test activations on newsgroups\n",
    "# !python multilayer_perceptron.py -d newsgroups -a relu -e 3\n",
    "# !python multilayer_perceptron.py -d newsgroups -a sigmoid -e 3\n",
    "# !python multilayer_perceptron.py -d newsgroups -a tanh -e 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
